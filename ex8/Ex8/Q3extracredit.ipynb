{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhorFWf2I0fB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tqdm\n",
        "import random\n",
        "import gym\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# customized weight initialization\n",
        "def customized_weights_init(m):\n",
        "    # compute the gain\n",
        "    gain = nn.init.calculate_gain('relu')\n",
        "    # init the convolutional layer\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        # init the params using uniform\n",
        "        nn.init.xavier_uniform_(m.weight, gain=gain)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "    # init the linear layer\n",
        "    if isinstance(m, nn.Linear):\n",
        "        # init the params using uniform\n",
        "        nn.init.xavier_uniform_(m.weight, gain=gain)\n",
        "        nn.init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "id": "cwnEf3HhNm_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepQNet(nn.Module):\n",
        "    def __init__(self, input_dim, num_hidden_layer, dim_hidden_layer, output_dim):\n",
        "        super(DeepQNet, self).__init__()\n",
        "\n",
        "        \"\"\"CODE HERE: construct your Deep neural network\n",
        "        \"\"\"\n",
        "         # define the input dimension\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        # define the hidden dimension\n",
        "        self.hidden_num = num_hidden_layer\n",
        "\n",
        "        # define the number of the hidden layers\n",
        "        self.hidden_dim = dim_hidden_layer\n",
        "\n",
        "        # define the output dimension\n",
        "        self.output_dim = output_dim\n",
        "      \n",
        "        self.dqn=nn.Sequential(nn.Linear(self.input_dim,self.hidden_dim),nn.ReLU(),nn.Linear(self.hidden_dim,self.hidden_dim),nn.ReLU(),\n",
        "                                                                   nn.Linear(self.hidden_dim,self.hidden_dim),nn.ReLU(),nn.Linear(self.hidden_dim,self.output_dim))\n",
        "                               \n",
        "\n",
        "    def forward(self,x):\n",
        "        y=self.dqn(x)\n",
        "        return y\n",
        "      \n"
      ],
      "metadata": {
        "id": "6IysUnxvNoJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "    \"\"\" Implement the Replay Buffer as a class, which contains:\n",
        "            - self._data_buffer (list): a list variable to store all transition tuples.\n",
        "            - add: a function to add new transition tuple into the buffer\n",
        "            - sample_batch: a function to sample a batch training data from the Replay Buffer\n",
        "    \"\"\"\n",
        "    def __init__(self, buffer_size):\n",
        "        \"\"\"Args:\n",
        "               buffer_size (int): size of the replay buffer\n",
        "        \"\"\"\n",
        "        # total size of the replay buffer\n",
        "        self.total_size = buffer_size\n",
        "\n",
        "        # create a list to store the transitions\n",
        "        self._data_buffer = []\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._data_buffer)\n",
        "\n",
        "    def add(self, obs, act, reward, next_obs, done):\n",
        "        # create a tuple\n",
        "        trans = (obs, act, reward, next_obs, done)\n",
        "\n",
        "        # interesting implementation\n",
        "        if self._next_idx >= len(self._data_buffer):\n",
        "            self._data_buffer.append(trans)\n",
        "        else:\n",
        "            self._data_buffer[self._next_idx] = trans\n",
        "\n",
        "        # increase the index\n",
        "        self._next_idx = (self._next_idx + 1) % self.total_size\n",
        "\n",
        "    def _encode_sample(self, indices):\n",
        "        \"\"\" Function to fetch the state, action, reward, next state, and done arrays.\n",
        "        \n",
        "            Args:\n",
        "                indices (list): list contains the index of all sampled transition tuples.\n",
        "        \"\"\"\n",
        "        # lists for transitions\n",
        "        obs_list, actions_list, rewards_list, next_obs_list, dones_list = [], [], [], [], []\n",
        "\n",
        "        # collect the data\n",
        "        for idx in indices:\n",
        "            # get the single transition\n",
        "            data = self._data_buffer[idx]\n",
        "            obs, act, reward, next_obs, d = data\n",
        "            # store to the list\n",
        "            obs_list.append(np.array(obs, copy=False))\n",
        "            actions_list.append(np.array(act, copy=False))\n",
        "            rewards_list.append(np.array(reward, copy=False))\n",
        "            next_obs_list.append(np.array(next_obs, copy=False))\n",
        "            dones_list.append(np.array(d, copy=False))\n",
        "        # return the sampled batch data as numpy arrays\n",
        "        return np.array(obs_list), np.array(actions_list), np.array(rewards_list), np.array(next_obs_list), np.array(\n",
        "            dones_list)\n",
        "\n",
        "    def sample_batch(self, batch_size):\n",
        "        \"\"\" Args:\n",
        "                batch_size (int): size of the sampled batch data.\n",
        "        \"\"\"\n",
        "        # sample indices with replaced\n",
        "        indices = [np.random.randint(0, len(self._data_buffer)) for _ in range(batch_size)]\n",
        "        return self._encode_sample(indices)"
      ],
      "metadata": {
        "id": "THvN20AcNrRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearSchedule(object):\n",
        "    \"\"\" This schedule returns the value linearly\"\"\"\n",
        "    def __init__(self, start_value, end_value, duration):\n",
        "        # start value\n",
        "        self._start_value = start_value\n",
        "        # end value\n",
        "        self._end_value = end_value\n",
        "        # time steps that value changes from the start value to the end value\n",
        "        self._duration = duration\n",
        "        # difference between the start value and the end value\n",
        "        self._schedule_amount = end_value - start_value\n",
        "        \n",
        "\n",
        "    def get_value(self, time):\n",
        "        # logic: if time > duration, use the end value, else use the scheduled value\n",
        "        \"\"\" CODE HERE: return the epsilon for each time step within the duration.\n",
        "        \"\"\"\n",
        "        if time>self._duration:\n",
        "            return self._end_value\n",
        "        else:\n",
        "            return self._start_value + self._schedule_amount * time / self._duration\n",
        "\n",
        "      \n",
        "        \n",
        "        \n",
        "\n",
        "       "
      ],
      "metadata": {
        "id": "ENlwQrMoNxgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent(object):\n",
        "    # initialize the agent\n",
        "    def __init__(self,\n",
        "                 params,\n",
        "                 ):\n",
        "        # save the parameters\n",
        "        self.params = params\n",
        "\n",
        "        # environment parameters\n",
        "        self.action_dim = params['action_dim']\n",
        "        self.obs_dim = params['observation_dim']\n",
        "\n",
        "        # executable actions\n",
        "        self.action_space = params['action_space']\n",
        "\n",
        "        # create behavior policy network\n",
        "        self.behavior_policy_net = DeepQNet(input_dim=params['observation_dim'],\n",
        "                                            num_hidden_layer=params['hidden_layer_num'],\n",
        "                                            dim_hidden_layer=params['hidden_layer_dim'],\n",
        "                                            output_dim=params['action_dim'])\n",
        "        # create target network\n",
        "        self.target_policy_net = DeepQNet(input_dim=params['observation_dim'],\n",
        "                                          num_hidden_layer=params['hidden_layer_num'],\n",
        "                                          dim_hidden_layer=params['hidden_layer_dim'],\n",
        "                                          output_dim=params['action_dim'])\n",
        "\n",
        "        # initialize target network with behavior network\n",
        "        self.behavior_policy_net.apply(customized_weights_init)\n",
        "        self.target_policy_net.load_state_dict(self.behavior_policy_net.state_dict())\n",
        "\n",
        "\n",
        "        # send the agent to a specific device: cpu or gpu\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        self.behavior_policy_net.to(self.device)\n",
        "        self.target_policy_net.to(self.device)\n",
        "        \n",
        "\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = torch.optim.Adam(self.behavior_policy_net.parameters(), lr=params['learning_rate'])\n",
        "\n",
        "    # get action\n",
        "    def get_action(self, obs, eps,bina):\n",
        "        if np.random.random() < eps:  # with probability eps, the agent selects a random action\n",
        "            action = self.action_space.sample()\n",
        "        else:  # with probability 1 - eps, the agent selects a greedy policy\n",
        "            obs = self._arr_to_tensor(obs).view(1, -1)\n",
        "            with torch.no_grad():\n",
        "                q_values = self.behavior_policy_net(obs)\n",
        "                action = q_values.max(dim=1)[1].item()\n",
        "        return action\n",
        "\n",
        "  # update behavior policy\n",
        "    def update_behavior_policy(self, batch_data):\n",
        "        # convert batch data to tensor and put them on device\n",
        "        batch_data_tensor = self._batch_to_tensor(batch_data)\n",
        "\n",
        "        # get the transition data\n",
        "        obs_tensor = batch_data_tensor['obs']\n",
        "        actions_tensor = batch_data_tensor['action']\n",
        "        next_obs_tensor = batch_data_tensor['next_obs']\n",
        "        rewards_tensor = batch_data_tensor['reward']\n",
        "        dones_tensor = batch_data_tensor['done']\n",
        "\n",
        "        \"\"\"CODE HERE:\n",
        "                Compute the predicted Q values using the behavior policy network\n",
        "        \"\"\"\n",
        "        # compute the q value estimation using the behavior network\n",
        "        q_current=self.behavior_policy_net(obs_tensor).gather(1,actions_tensor)\n",
        "        # next_state_values=torch.zeros(self.params['batch_size'],device=self.device)\n",
        "        next_state_values=self.target_policy_net(next_obs_tensor).max(1)[0].view(-1,1)\n",
        "       \n",
        "        # compute the TD target using the target network\n",
        "        TD_target=rewards_tensor+self.params['gamma']*next_state_values*(1-dones_tensor)\n",
        "      \n",
        "        # compute the loss\n",
        "        td_loss=torch.nn.functional.mse_loss(q_current,TD_target)\n",
        "   \n",
        "        # minimize the loss\n",
        "        self.optimizer.zero_grad()\n",
        "        td_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return td_loss.item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # update update target policy\n",
        "    def update_target_policy(self):\n",
        "        # hard update\n",
        "        \"\"\"CODE HERE: \n",
        "                Copy the behavior policy network to the target network\n",
        "        \"\"\"\n",
        "        self.target_policy_net.load_state_dict(self.behavior_policy_net.state_dict())\n",
        "\n",
        "    # load trained model\n",
        "    def load_model(self, model_file):\n",
        "        # load the trained model\n",
        "        self.behavior_policy_net.load_state_dict(torch.load(model_file, map_location=self.device))\n",
        "        self.behavior_policy_net.eval()\n",
        "\n",
        "    # auxiliary functions\n",
        "    def _arr_to_tensor(self, arr):\n",
        "        arr = np.array(arr)\n",
        "        arr_tensor = torch.from_numpy(arr).float().to(self.device)\n",
        "        return arr_tensor\n",
        "\n",
        "    def _batch_to_tensor(self, batch_data):\n",
        "        # store the tensor\n",
        "        batch_data_tensor = {'obs': [], 'action': [], 'reward': [], 'next_obs': [], 'done': []}\n",
        "        # get the numpy arrays\n",
        "        obs_arr, action_arr, reward_arr, next_obs_arr, done_arr = batch_data\n",
        "        # convert to tensors\n",
        "        batch_data_tensor['obs'] = torch.tensor(obs_arr, dtype=torch.float32).to(self.device)\n",
        "        batch_data_tensor['action'] = torch.tensor(action_arr).long().view(-1, 1).to(self.device)\n",
        "        batch_data_tensor['reward'] = torch.tensor(reward_arr, dtype=torch.float32).view(-1, 1).to(self.device)\n",
        "        batch_data_tensor['next_obs'] = torch.tensor(next_obs_arr, dtype=torch.float32).to(self.device)\n",
        "        batch_data_tensor['done'] = torch.tensor(done_arr, dtype=torch.float32).view(-1, 1).to(self.device)\n",
        "\n",
        "        return batch_data_tensor"
      ],
      "metadata": {
        "id": "kFWg0CXhJ0sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dqn_agent(env, params,extra_env,render):\n",
        "    # create the DQN agent\n",
        "    my_agent = DQNAgent(params)\n",
        "\n",
        "    # create the epsilon-greedy schedule\n",
        "    my_schedule = LinearSchedule(start_value=params['epsilon_start_value'],\n",
        "                                 end_value=params['epsilon_end_value'],\n",
        "                                 duration=params['epsilon_duration'])\n",
        "\n",
        "    # create the replay buffer\n",
        "    replay_buffer = ReplayBuffer(params['replay_buffer_size'])\n",
        "\n",
        "    # training variables\n",
        "    episode_t = 0\n",
        "    rewards = []\n",
        "    train_returns = []\n",
        "    train_loss = []\n",
        "    loss = 0\n",
        "\n",
        "    # reset the environment\n",
        "    obs = env.reset()\n",
        "\n",
        "    # start training\n",
        "    pbar = tqdm.trange(params['total_training_time_step'])\n",
        "    last_best_return = 0\n",
        "    for t in pbar:\n",
        "        # scheduled epsilon at time step t\n",
        "        eps_t = my_schedule.get_value(t)\n",
        "        # get one epsilon-greedy action\n",
        "        action = my_agent.get_action(obs, eps_t,False)\n",
        "\n",
        "        # step in the environment\n",
        "        next_obs, reward, done, _ = env.step(action)\n",
        "       \n",
        "\n",
        "        # add to the buffer\n",
        "        replay_buffer.add(obs, action, reward, next_obs, done)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        # check termination\n",
        "        if done or episode_t == params['max_time_step_per_episode'] - 1:\n",
        "            # compute the return\n",
        "            G = 0\n",
        "            for r in reversed(rewards):\n",
        "                G = r + params['gamma'] * G\n",
        "\n",
        "            if G > last_best_return:\n",
        "                torch.save(my_agent.behavior_policy_net.state_dict(), f\"./{params['model_name']}\")\n",
        "\n",
        "            # store the return\n",
        "            train_returns.append(G)\n",
        "            episode_idx = len(train_returns)\n",
        "\n",
        "            # print the information\n",
        "            pbar.set_description(\n",
        "                f\"Ep={episode_idx} | \"\n",
        "                f\"G={np.mean(train_returns[-10:]) if train_returns else 0:.2f} | \"\n",
        "                f\"Eps={eps_t}\"\n",
        "            )\n",
        "\n",
        "            # reset the environment\n",
        "            episode_t, rewards = 0, []\n",
        "            obs= env.reset()\n",
        "        else:\n",
        "            # increment\n",
        "            obs = next_obs\n",
        "            episode_t += 1\n",
        "\n",
        "        if t > params['start_training_step']:\n",
        "            # update the behavior model\n",
        "            if not np.mod(t, params['freq_update_behavior_policy']):\n",
        "                \"\"\" CODE HERE:\n",
        "                    Update the behavior policy network\n",
        "                \"\"\"\n",
        "                batch_data=replay_buffer.sample_batch(params['batch_size'])\n",
        "                \n",
        "                train_loss.append(my_agent.update_behavior_policy(batch_data))\n",
        "\n",
        "            # update the target model\n",
        "            if not np.mod(t, params['freq_update_target_policy']):\n",
        "                \"\"\" CODE HERE:\n",
        "                    Update the behavior policy network\n",
        "                \"\"\"\n",
        "                my_agent.update_target_policy()\n",
        "\n",
        "        if render and t>100 and t%params['freq_render_env'] ==0:\n",
        "          obs=env.reset()\n",
        "          done=False\n",
        "\n",
        "          fig,ax=plt.subplots()\n",
        "          fig.show()\n",
        "          fig.canvas.draw()\n",
        "\n",
        "          extra_env.reset()\n",
        "          num_trial=1\n",
        "          max_time_steps=60\n",
        "          while not done:\n",
        "            action,q_values=my_agent.get_action(obs,0,True)\n",
        "            obs,_,done,_=env.step(action)\n",
        "            ax.imshow(env.render('rgb_array'))\n",
        "            fig.canvas.draw()\n",
        "            num_trial+=1\n",
        "            if num_trial==max_time_steps:\n",
        "              break\n",
        "\n",
        "\n",
        "\n",
        "    # save the results\n",
        "    return train_returns, train_loss"
      ],
      "metadata": {
        "id": "CiV42-KmJ7ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # set the random seed\n",
        "    np.random.seed(1234)\n",
        "    random.seed(1234)\n",
        "    torch.manual_seed(1234)\n",
        "\n",
        "    # create environment\n",
        "    my_env = gym.make(\"CartPole-v0\")\n",
        "    extra_env=gym.make('CartPole-v0')\n",
        "\n",
        "    # create training parameters\n",
        "    train_parameters = {\n",
        "      'observation_dim': 4,\n",
        "        'action_dim': 2,\n",
        "        'action_space': my_env.action_space,\n",
        "        'hidden_layer_num': 2,\n",
        "        'hidden_layer_dim': 128,\n",
        "        'gamma': 0.99999,\n",
        "        \n",
        "        'max_time_step_per_episode': 200,\n",
        "\n",
        "        'total_training_time_step': 1_000_000,\n",
        "\n",
        "        'epsilon_start_value': 1.0,\n",
        "        'epsilon_end_value': 0.01,\n",
        "        'epsilon_duration': 500_000,\n",
        "\n",
        "        'replay_buffer_size': 500_000,\n",
        "        'start_training_step': 2000,\n",
        "        'freq_update_behavior_policy': 1000,\n",
        "        'freq_update_target_policy': 10000,\n",
        "        'freq_render_env':400_000,\n",
        "\n",
        "        'batch_size': 128,\n",
        "        'learning_rate': 1e-3,\n",
        "\n",
        "        'model_name': \"cartpole_v1.pt\"\n",
        "    }\n",
        "    run_trial = 5\n",
        "    train_returns={}\n",
        "    train_loss={}\n",
        "    for i in range(run_trial):\n",
        "        render= i==0\n",
        "        train_returns[i], train_loss[i] = train_dqn_agent(my_env, train_parameters,extra_env,render)\n",
        "        min_value=min([len(return_value) for return_value in train_returns.values()])\n",
        "\n",
        "    train_returns_clip=[]\n",
        "    for trial_num,returns in train_returns.items():\n",
        "        train_returns_clip.append(returns[:min_value])"
      ],
      "metadata": {
        "id": "b-uMUb8vJ_Kk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}