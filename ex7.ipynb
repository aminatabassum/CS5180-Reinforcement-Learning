{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c8fb627",
   "metadata": {},
   "source": [
    "# Please install the following python libraries\n",
    "- python3: https://www.python.org/\n",
    "- numpy: https://numpy.org/install/\n",
    "- tqdm: https://github.com/tqdm/tqdm#installation\n",
    "- matplotlib: https://matplotlib.org/stable/users/installing/index.html\n",
    "\n",
    "If you encounter the error: \"IProgress not found. Please update jupyter & ipywidgets\"\n",
    "    \n",
    "Please install the ipywidgets as follows:\n",
    "\n",
    "    with pip, do\n",
    "    - pip install ipywidgets\n",
    "    \n",
    "    with conda, do\n",
    "    - conda install -c conda-forge ipywidgets\n",
    "    \n",
    "Restart your notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e69a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c2a45",
   "metadata": {},
   "source": [
    "# Q3: Solving Four Rooms using semi-gradient SARSA with state aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4adaa389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Four Rooms Environment Implementation\n",
    "\"\"\"\n",
    "class FourRooms(object):\n",
    "    def __init__(self):\n",
    "        # We define the grid for the Four Rooms domain\n",
    "        self.grid = np.array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "        # We define the observation space consisting of all empty cells\n",
    "        # Note: We have to flip the coordinates from (row_idx, column_idx) -> (x, y),\n",
    "        # where x = column_idx, y = 10 - row_idx\n",
    "        self.observation_space = np.argwhere(self.grid == 0.0).tolist()  # Fine all empty cells\n",
    "        self.observation_space = self.arr_coords_to_four_room_coords(self.observation_space)\n",
    "\n",
    "        # We define the action space\n",
    "        self.action_space = {'up': np.array([0, 1]),\n",
    "                             'down': np.array([0, -1]),\n",
    "                             'left': np.array([-1, 0]),\n",
    "                             'right': np.array([1, 0])}\n",
    "        self.action_names = ['up', 'down', 'left', 'right']\n",
    "\n",
    "        # We define the start location\n",
    "        self.start_location = [0, 0]\n",
    "\n",
    "        # We define the goal location\n",
    "        self.goal_location = [10, 10]\n",
    "\n",
    "        # We find all wall cells\n",
    "        self.walls = np.argwhere(self.grid == 1.0).tolist()  # find all wall cells\n",
    "        self.walls = self.arr_coords_to_four_room_coords(self.walls)  # convert to Four Rooms coordinates\n",
    "\n",
    "        # This is an episodic task, we define a timeout: maximal time steps = 459\n",
    "        self.max_time_steps = 459\n",
    "\n",
    "        # We define other useful variables\n",
    "        self.agent_location = None  # track the agent's location in one episode.\n",
    "        self.action = None  # track the agent's action\n",
    "        self.t = 0  # track the current time step in one episode\n",
    "\n",
    "    @staticmethod\n",
    "    def arr_coords_to_four_room_coords(arr_coords_list):\n",
    "        \"\"\"\n",
    "        Function converts the array coordinates to the Four Rooms coordinates (i.e, The origin locates at bottom left).\n",
    "        E.g., The coordinates (0, 0) in the numpy array is mapped to (0, 10) in the Four Rooms coordinates.\n",
    "        Args:\n",
    "            arr_coords_list (list): a list variable consists of tuples of locations in the numpy array\n",
    "\n",
    "        Return:\n",
    "            four_room_coords_list (list): a list variable consists of tuples of converted locations in the\n",
    "                                          Four Rooms environment.\n",
    "        \"\"\"\n",
    "        # Note: We have to flip the coordinates from (row_idx, column_idx) -> (x, y),\n",
    "        # where x = column_idx, y = 10 - row_idx\n",
    "        four_room_coords_list = [(column_idx, 10 - row_idx) for (row_idx, column_idx) in arr_coords_list]\n",
    "        return four_room_coords_list\n",
    "\n",
    "    def reset(self):\n",
    "        # We reset the agent's location to the start location\n",
    "        self.agent_location = self.start_location\n",
    "\n",
    "        # We reset the timeout tracker to be 0\n",
    "        self.t = 0\n",
    "\n",
    "        # We set the information\n",
    "        info = {}\n",
    "        return self.agent_location, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action (string): a string variable (i.e., \"UP\"). All feasible values are [\"up\", \"down\", \"left\", \"right\"].\n",
    "        \"\"\"\n",
    "        # With probability 0.8, the agent takes the correct direction.\n",
    "        # With probability 0.2, the agent takes one of the two perpendicular actions.\n",
    "        # For example, if the correct action is \"LEFT\", then\n",
    "        #     - With probability 0.8, the agent takes action \"LEFT\";\n",
    "        #     - With probability 0.1, the agent takes action \"UP\";\n",
    "        #     - With probability 0.1, the agent takes action \"DOWN\".\n",
    "        if np.random.uniform() < 0.2:\n",
    "            if action == \"left\" or action == \"right\":\n",
    "                action = np.random.choice([\"up\", \"down\"], 1)[0]\n",
    "            else:\n",
    "                action = np.random.choice([\"right\", \"left\"], 1)[0]\n",
    "\n",
    "        # Convert the agent's location to array\n",
    "        loc_arr = np.array(self.agent_location)\n",
    "\n",
    "        # Convert the action name to movement array\n",
    "        act_arr = self.action_space[action]\n",
    "\n",
    "        # Compute the agent's next location\n",
    "        next_agent_location = np.clip(loc_arr + act_arr,\n",
    "                                      a_min=np.array([0, 0]),\n",
    "                                      a_max=np.array([10, 10])).tolist()\n",
    "\n",
    "        # Check if the agent crashes into walls, it stays at the current location.\n",
    "        if tuple(next_agent_location) in self.walls:\n",
    "            next_agent_location = self.agent_location\n",
    "\n",
    "        # Compute the reward\n",
    "        reward = 1.0 if next_agent_location == self.goal_location else 0.0\n",
    "\n",
    "        # Check the termination\n",
    "        # If the agent reaches the goal, reward = 1, done = True\n",
    "        # If the time steps reaches the maximal number, reward = 0, done = True.\n",
    "        if reward == 1.0 or self.t == self.max_time_steps:\n",
    "            terminated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "\n",
    "        # Update the agent's location, action and time step trackers\n",
    "        self.agent_location = next_agent_location\n",
    "        self.action = action\n",
    "        self.t += 1\n",
    "\n",
    "        return next_agent_location, reward, terminated, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        # plot the agent and the goal\n",
    "        # empty cell = 0\n",
    "        # wall cell = 1\n",
    "        # agent cell = 2\n",
    "        # goal cell = 3\n",
    "        plot_arr = self.grid.copy()\n",
    "        plot_arr[10 - self.agent_location[1], self.agent_location[0]] = 2\n",
    "        plot_arr[10 - self.goal_location[1], self.goal_location[0]] = 3\n",
    "        plt.clf()\n",
    "        plt.title(f\"state={self.agent_location}, act={self.action}\")\n",
    "        plt.imshow(plot_arr)\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    @staticmethod\n",
    "    def test():\n",
    "        my_env = FourRooms()\n",
    "        state, _ = my_env.reset()\n",
    "\n",
    "        for _ in range(100):\n",
    "            action = np.random.choice(list(my_env.action_space.keys()), 1)[0]\n",
    "\n",
    "            next_state, reward, done, _, _ = my_env.step(action)\n",
    "            my_env.render()\n",
    "\n",
    "            if done:\n",
    "                state, _ = my_env.reset()\n",
    "            else:\n",
    "                state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5d4406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(arr_list, legend_list, color_list, ylabel, fig_title):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        arr_list (list): list of results arrays to plot\n",
    "        legend_list (list): list of legends corresponding to each result array\n",
    "        color_list (list): list of color corresponding to each result array\n",
    "        ylabel (string): label of the Y axis\n",
    "\n",
    "        Note that, make sure the elements in the arr_list, legend_list and color_list are associated with each other correctly.\n",
    "        Do not forget to change the ylabel for different plots.\n",
    "    \"\"\"\n",
    "    # set the figure type\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # PLEASE NOTE: Change the labels for different plots\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "\n",
    "    # ploth results\n",
    "    h_list = []\n",
    "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
    "        # compute the standard error\n",
    "        arr_err = arr.std(axis=0) / np.sqrt(arr.shape[0])\n",
    "        # plot the mean\n",
    "        h, = ax.plot(range(arr.shape[1]), arr.mean(axis=0), color=color, label=legend)\n",
    "        # plot the confidence band\n",
    "        arr_err *= 1.96\n",
    "        ax.fill_between(range(arr.shape[1]), arr.mean(axis=0) - arr_err, arr.mean(axis=0) + arr_err, alpha=0.3,\n",
    "                        color=color)\n",
    "        # save the plot handle\n",
    "        h_list.append(h)\n",
    "\n",
    "    # plot legends\n",
    "    ax.set_title(f\"{fig_title}\")\n",
    "    ax.legend(handles=h_list)\n",
    "\n",
    "    # save the figure\n",
    "    plt.savefig(f\"{fig_title}.png\", dpi=200)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1118c4",
   "metadata": {},
   "source": [
    "# Q3 - (a): Implement the semi-gradient SARSA\n",
    "\n",
    "As described in the question, you are asked to implement the semi-gradient SARSA with a very simple state aggregation strategy. That is aggregating both states and actions to itself. Indeed, this will have similar results as applying *tabular* SARSA directly. \n",
    "\n",
    "**Please implement the following state aggregation strategy**\n",
    "\n",
    "- For each state, its aggregated state is itself. E.g. [0, 0] is aggregated to [0, 0] only. \n",
    "- For each action, its aggregated action is also itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53298add",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemiGradientSARSAAgent(object):\n",
    "    def __init__(self, env, info):\n",
    "        \"\"\"\n",
    "        Function to initialize the semi-gradient SARSA agent\n",
    "        Args:\n",
    "            env: the environment that the agent interacts with\n",
    "            info (dict): a dictionary variable contains all necessary parameters.\n",
    "\n",
    "        Note that: In this question, we will implement a simple state aggregation strategies.\n",
    "                   Specifically, we design the following function approximation:\n",
    "                   1. Feature: we use the one-hot encoding to compute the feature for each state-action pair.\n",
    "                               E.g., state = [0, 0] and action = \"Up\" (state-action pair) will correspond to\n",
    "                               a unique one-hot representation $f(s, a) = [0, 0, 0, 1, 0, ..., 0]$.\n",
    "                   2. Weights: we define a weight vector $w$ having the sample shape as the feature vector.\n",
    "                               Specifically, the Q(s, a) can be estimated by Q(s, a) = w^{T} * f(s, a)\n",
    "\n",
    "        Importantly, as described in the question, we only aggregate the states.\n",
    "        \"\"\"\n",
    "        # Store the environment\n",
    "        self.env = env\n",
    "\n",
    "        \"\"\" Learning parameters for semi-gradient SARSA \"\"\"\n",
    "        # Store the number of learning episodes\n",
    "        self.episode_num = info['episode_num']\n",
    "\n",
    "        # Store the update step size alpha\n",
    "        self.alpha = info['alpha']\n",
    "\n",
    "        # Store the discount factor\n",
    "        self.gamma = info['gamma']\n",
    "\n",
    "        # Initialize the epsilon\n",
    "        self.epsilon = info['epsilon']\n",
    "\n",
    "        # Store the other hyerparameters\n",
    "        self.params = info\n",
    "\n",
    "        \"\"\" State aggregation for semi-gradient SARSA \"\"\"\n",
    "        # Compute the total number of state after the aggregation\n",
    "        self.state_space, self.state_num = self.create_state_aggregation()\n",
    "\n",
    "        # Compute the total number of the actions\n",
    "        self.action_num = len(self.env.action_names)\n",
    "\n",
    "        \"\"\" Function approximation for semi-gradient SARSA\"\"\"\n",
    "        # We create a weight with shape |S| * |A|\n",
    "        self.weights_fn = np.zeros((self.state_num * self.action_num))\n",
    "\n",
    "        # We construct a numpy array that contains the one-hot features for all state-action pairs.\n",
    "        # The size is (|S| * |A|) x (|S| * |A|).\n",
    "        # Each i-th row is the one-hot encoding for state-action pair with index i.\n",
    "        self.feature_arr = np.eye(self.state_num * self.action_num)\n",
    "\n",
    "    def create_state_aggregation(self):\n",
    "        \"\"\"\n",
    "        Function that returns the aggregated state space and the number of the aggregated states.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"CODE HERE: your state aggregation strategy.\n",
    "           \n",
    "           You have to return:\n",
    "            1. the aggregated state space (Any data structure that you find it easier to render the index of the \n",
    "               aggregated state.)\n",
    "            2. the number of the aggregated states (int)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # compute the aggregated state space based on the state aggregation strategy\n",
    "        aggregated_state_space = self.env.observation_space\n",
    "\n",
    "        # compute the number of the state in the aggregated state space\n",
    "        aggregate_state_num = len(aggregated_state_space)\n",
    "\n",
    "        return aggregated_state_space, aggregate_state_num\n",
    "\n",
    "    def _aggregate_state_idx(self, state):\n",
    "        \"\"\"\n",
    "        Function returns the index of aggregated state given an original state\n",
    "\n",
    "        Args:\n",
    "            state (list): original state\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"CODE HERE: based on your state aggregation, return the index of the aggregated state given an original\n",
    "           state. \n",
    "           \n",
    "           You have to return:\n",
    "           1. index (int) of the aggregated state given the original state\n",
    "        \"\"\"\n",
    "        state=tuple(state)\n",
    "        state_idx = self.env.observation_space.index(state)\n",
    "        return state_idx\n",
    "\n",
    "    def _aggregate_action_idx(self, action):\n",
    "        \"\"\"\n",
    "        Function returns the index of aggregated action.\n",
    "        Args:\n",
    "            action (string): name of the action\n",
    "\n",
    "        To be simple, here, one action only aggregates to itself\n",
    "        \"\"\"\n",
    "        return self.env.action_names.index(action)\n",
    "\n",
    "    def _get_state_action_feature(self, state, action):\n",
    "        \"\"\"\n",
    "        Function that returns the one-hot feature given a state-action pair.\n",
    "\n",
    "        Args:\n",
    "            state (list): original state\n",
    "            action (string): name of the action\n",
    "        \"\"\"\n",
    "        # Get the unique index of the aggregated state\n",
    "        state_index = self._aggregate_state_idx(state)\n",
    "        # Get the unique index of the aggregated action\n",
    "        action_index = self._aggregate_action_idx(action)\n",
    "        # Compute the state(aggregated)-action index\n",
    "        state_action_index = self.state_num * action_index + state_index\n",
    "        # Get the one-hot feature of the state\n",
    "        return self.feature_arr[state_action_index]\n",
    "\n",
    "    def function_approximation(self, state, action):\n",
    "        \"\"\"\n",
    "        Function that computes the Q value given a state-action pair using linear function approximation.\n",
    "        Args:\n",
    "            state (list): original state\n",
    "            action (string): name of the action\n",
    "        \"\"\"\n",
    "        state_action_feature = self._get_state_action_feature(state, action)\n",
    "        return np.matmul(state_action_feature.T, self.weights_fn)\n",
    "\n",
    "    def render_q_value(self, state, action):\n",
    "        \"\"\"\n",
    "        Function that returns the Q value given a state-action pair\n",
    "\n",
    "        Args:\n",
    "            state (list): original state\n",
    "            action (string): name of the action\n",
    "        \"\"\"\n",
    "        return self.function_approximation(state, action)\n",
    "\n",
    "\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        \"\"\"\n",
    "        Function implements the epsilon-greedy policy\n",
    "        Args:\n",
    "            state (list): original state\n",
    "        \"\"\"\n",
    "        \"\"\"CODE HERE:\n",
    "           implement the epsilon-greedy policy using function approximation. Break ties if happens \n",
    "           \n",
    "           You should return:\n",
    "           1. name of the action (string)\n",
    "        \"\"\"\n",
    "        # q_values=self.feature_arr.T * self.weights_fn\n",
    "        if random.random() < self.epsilon:  # with p = epsilon, we randomly sample an action\n",
    "            action= random.sample(self.env.action_names, 1)[0]\n",
    "        else:  # With p = 1 - epsilon, derive a greedy policy from the Q table\n",
    "            state_idx = self._aggregate_state_idx(state)\n",
    "            q_values = self.q_table(state_idx)\n",
    "            max_action_list = np.where(q_values == q_values.max())[0].tolist()\n",
    "            max_action_idx = random.sample(max_action_list, 1)[0]\n",
    "            action= self.env.action_names[max_action_idx]  # break ties\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def update_weights(self, s, a, r, s_prime, a_prime):\n",
    "        \"\"\"\n",
    "        Function that updates the weights using semi-gradients\n",
    "\n",
    "        Args:\n",
    "            s (list): original state\n",
    "            a (string): action name\n",
    "            r (float): reward\n",
    "            s_prime (list): original next state\n",
    "            a_prime (string): next action name\n",
    "        \"\"\"\n",
    "        \"\"\" CODE HERE:\n",
    "            implement the update of the semi-gradient SARSA\n",
    "            \n",
    "            You should update \"self.weights_fn\"\n",
    "        \"\"\"\n",
    "        # check if s_prime is the termination state\n",
    "\n",
    "        # # q_values=self.feature_arr.T * self.weights_fn\n",
    "        # s=tuple(s)\n",
    "        # s_prime=tuple(s_prime)\n",
    "        # s_id=self._aggregate_state_idx(s)\n",
    "        # a_id=self._aggregate_action_idx(a)\n",
    "        # ns_idx=self._aggregate_state_idx(s_prime)\n",
    "        # na_id=self._aggregate_action_idx(a_prime)\n",
    "        # gra=np.zeros((self.action_num,self.state_num*self.action_num))\n",
    "        # gra[a_id,s_id+a_id]=1\n",
    "        # if s_prime==self.env.goal_location:\n",
    "        #     weights=self.weights_fn[a_id:ns_idx+na_id]\n",
    "        #     weights += self.alpha*(r-self.q_table_w(s_id))*gra\n",
    "        #     self.weights_fn[a_id:ns_idx+na_id]=weights\n",
    "            \n",
    "\n",
    "        # else:\n",
    "        #     action=self.epsilon_greedy_policy(s_prime)\n",
    "        #     action_id=self._aggregate_action_idx(action)\n",
    "        #     weights=self.weights_fn[a_id:ns_idx+na_id]\n",
    "        #     print(gra.shape)\n",
    "        #     weights += self.alpha*(r+ self.gamma*self.q_table_w(ns_idx)-self.q_table_w(s_id))*gra\n",
    "        #     self.weights_fn[a_id:ns_idx+na_id]=weights\n",
    "        #     s=s_prime\n",
    "        #     a=action\n",
    "\n",
    "\n",
    "           \n",
    "\n",
    "        \n",
    "        \n",
    "        # update the weights\n",
    "\n",
    "    def run(self):\n",
    "        # Save the discounted return for each episode\n",
    "        discounted_returns = []\n",
    "\n",
    "        # Semi-gradient SARSA starts\n",
    "        for ep in tqdm.trange(self.episode_num):\n",
    "            \"\"\"CODE HERE:\n",
    "               Implement the pseudocode of the Semi-gradient SARSA\n",
    "            \"\"\"\n",
    "            # Reset the agent to initial STATE at the beginning of every episode\n",
    "            initial_state,_=self.env.reset()\n",
    "            initial_state=tuple(initial_state)\n",
    "\n",
    "            # Render an ACTION based on the initial STATE\n",
    "            action=self.epsilon_greedy_policy(initial_state)\n",
    "\n",
    "            # Store rewards to compute return G for the current episode.\n",
    "            reward_list = []\n",
    "            \n",
    "            # Loop the episode\n",
    "            for t in range(self.env.max_time_steps):\n",
    "                # Take the ACTION and observe REWARD and NEXT STATE\n",
    "                next_s,reward,done,_,_=self.env.step(action)\n",
    "                next_s=tuple(next_s)\n",
    "\n",
    "                # Given the NEXT STATE, choose the NEXT ACTION\n",
    "                new_action=self.epsilon_greedy_policy(next_s)\n",
    "\n",
    "                # Update the weights of the function using semi-gradient SARSA\n",
    "                weights=self.update_weights(initial_state,action,reward,next_s,new_action)\n",
    "                # Using STATE, ACTION, REWARD, NEXT STATE, NEXT ACTION\n",
    "                \n",
    "                \"\"\"DO NOT CHANGE BELOW\"\"\"\n",
    "                # Save the reward for plotting\n",
    "                reward_list.append(reward)\n",
    "\n",
    "                # Reset the environment\n",
    "                if done:\n",
    "                    break\n",
    "                else:\n",
    "                    state = next_state\n",
    "                    action = next_action\n",
    "\n",
    "            # compute the discounted return for the current episode\n",
    "            G = 0\n",
    "            for reward in reversed(reward_list):\n",
    "                G = reward + self.gamma * G\n",
    "            discounted_returns.append(G)\n",
    "\n",
    "        return discounted_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea88e472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 416)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (80,) (4,416) (80,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/amina/Documents/Sem3/RL/ex7.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# run semi-gradient SARSA\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m my_agent \u001b[39m=\u001b[39m SemiGradientSARSAAgent(my_env, params)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m res \u001b[39m=\u001b[39m my_agent\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# save result for each running trial\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m results\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39marray(res))\n",
      "\u001b[1;32m/home/amina/Documents/Sem3/RL/ex7.ipynb Cell 8\u001b[0m in \u001b[0;36mSemiGradientSARSAAgent.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X10sZmlsZQ%3D%3D?line=253'>254</a>\u001b[0m new_action\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon_greedy_policy(next_s)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X10sZmlsZQ%3D%3D?line=255'>256</a>\u001b[0m \u001b[39m# Update the weights of the function using semi-gradient SARSA\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X10sZmlsZQ%3D%3D?line=256'>257</a>\u001b[0m weights\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_weights(initial_state,action,reward,next_s,new_action)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X10sZmlsZQ%3D%3D?line=257'>258</a>\u001b[0m \u001b[39m# Using STATE, ACTION, REWARD, NEXT STATE, NEXT ACTION\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X10sZmlsZQ%3D%3D?line=259'>260</a>\u001b[0m \u001b[39m\"\"\"DO NOT CHANGE BELOW\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m/home/amina/Documents/Sem3/RL/ex7.ipynb Cell 8\u001b[0m in \u001b[0;36mSemiGradientSARSAAgent.update_weights\u001b[0;34m(self, s, a, r, s_prime, a_prime)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X10sZmlsZQ%3D%3D?line=213'>214</a>\u001b[0m weights\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights_fn[a_id:ns_idx\u001b[39m+\u001b[39mna_id]\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X10sZmlsZQ%3D%3D?line=214'>215</a>\u001b[0m \u001b[39mprint\u001b[39m(gra\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X10sZmlsZQ%3D%3D?line=215'>216</a>\u001b[0m weights \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha\u001b[39m*\u001b[39m(r\u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_table_w(ns_idx)\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_table_w(s_id))\u001b[39m*\u001b[39mgra\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X10sZmlsZQ%3D%3D?line=216'>217</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights_fn[a_id:ns_idx\u001b[39m+\u001b[39mna_id]\u001b[39m=\u001b[39mweights\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X10sZmlsZQ%3D%3D?line=217'>218</a>\u001b[0m s\u001b[39m=\u001b[39ms_prime\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (80,) (4,416) (80,) "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set the random seed\n",
    "    np.random.seed(1234)\n",
    "    random.seed(1234)\n",
    "\n",
    "    # set hyper-parameters\n",
    "    params = {\n",
    "        \"episode_num\": 100,\n",
    "        \"alpha\": 0.1,\n",
    "        \"gamma\": 0.99,\n",
    "        \"epsilon\": 0.1\n",
    "    }\n",
    "\n",
    "    # set running trials. You can try run_trial = 5 for debugging\n",
    "    run_trial = 10\n",
    "\n",
    "    # run multiple trials\n",
    "    results = []\n",
    "    for _ in range(run_trial):\n",
    "        # create the environment\n",
    "        my_env = FourRooms()\n",
    "\n",
    "        # run semi-gradient SARSA\n",
    "        my_agent = SemiGradientSARSAAgent(my_env, params)\n",
    "        res = my_agent.run()\n",
    "\n",
    "        # save result for each running trial\n",
    "        results.append(np.array(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "008c2611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amina/.local/lib/python3.8/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/amina/.local/lib/python3.8/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/home/amina/.local/lib/python3.8/site-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/amina/Documents/Sem3/RL/ex7.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plot_curves([np\u001b[39m.\u001b[39;49marray(results)], [\u001b[39m\"\u001b[39;49m\u001b[39msemi-gradient SARSA\u001b[39;49m\u001b[39m\"\u001b[39;49m], [\u001b[39m\"\u001b[39;49m\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mAveraged discounted return\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mQ3 - (a): semi-gradient SARSA\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/home/amina/Documents/Sem3/RL/ex7.ipynb Cell 9\u001b[0m in \u001b[0;36mplot_curves\u001b[0;34m(arr_list, legend_list, color_list, ylabel, fig_title)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m arr_err \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mstd(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msqrt(arr\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# plot the mean\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m h, \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39mplot(\u001b[39mrange\u001b[39m(arr\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]), arr\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), color\u001b[39m=\u001b[39mcolor, label\u001b[39m=\u001b[39mlegend)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# plot the confidence band\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amina/Documents/Sem3/RL/ex7.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m arr_err \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1.96\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/QAAAKsCAYAAABCj/ZMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzde3CV9Z348c8BIVEkp14AUZGLKIpWEfACSq23uNq1UjsjTltRq7vaGwW0XVlbbVEH6229FZQVyrKrlpGq43ZRSdUWb52WWxGl6IoawCACJUFUoMn5/eGQX2MC5qHnEL7u6zVzZnK+Oc9zPvnzneeWKxQKhQAAAACS0q6tBwAAAACyE/QAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkKA2Dfo5c+bEOeecE/vvv3/kcrl47LHHPnWb3/3udzFo0KAoLy+PPn36xL333rsTJgUAAIBdS5sG/caNG+Poo4+Oe+65p1Wff/PNN+Pss8+OYcOGxYIFC+Jf//VfY9SoUfGrX/2qxJMCAADAriVXKBQKbT1EREQul4tHH300hg8fvs3P/Mu//Es8/vjjsWTJksa1K664Iv70pz/FSy+9tDPGBAAAgF3Cbm09QBYvvfRSVFZWNlk788wzY8qUKbFly5bo0KFDs202bdoUmzZtanzf0NAQ69ati3322SdyuVzJZwYAAOD/tkKhEBs2bIj9998/2rUr3onySQX9qlWrolu3bk3WunXrFn/9619jzZo10b1792bbTJgwIX7605/urBEBAACgRcuXL48DDzywaPtLKugjotlR9a1XDGzraPu4ceNi7Nixje9ra2vjoIMOiuXLl0dFRUXpBgUAAICIqKurix49ekTnzp2Lut+kgn6//faLVatWNVlbvXp17LbbbrHPPvu0uE1ZWVmUlZU1W6+oqBD0AAAA7DTFvuw7qefQDxkyJKqqqpqszZ49OwYPHtzi9fMAAADwWdWmQf/+++/HwoULY+HChRHx8WPpFi5cGNXV1RHx8enyI0eObPz8FVdcEW+//XaMHTs2lixZElOnTo0pU6bEVVdd1SbzAwAAQFtp01Pu586dG6ecckrj+63Xul900UUxbdq0qKmpaYz7iIjevXvHrFmzYsyYMfHzn/889t9//7jrrrviq1/96k6fHQAAANrSLvMc+p2lrq4u8vl81NbWuoYeAACAkitVhyZ1DT0AAADwMUEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAlq86CfOHFi9O7dO8rLy2PQoEHx3HPPbffzDzzwQBx99NGxxx57RPfu3eOSSy6JtWvX7qRpAQAAYNfQpkE/Y8aMGD16dFxzzTWxYMGCGDZsWJx11llRXV3d4ueff/75GDlyZFx66aXxyiuvxMMPPxx//OMf47LLLtvJkwMAAEDbatOgv/322+PSSy+Nyy67LA4//PC44447okePHjFp0qQWP//73/8+evXqFaNGjYrevXvHSSedFJdffnnMnTt3J08OAAAAbavNgn7z5s0xb968qKysbLJeWVkZL774YovbDB06NFasWBGzZs2KQqEQ7777bsycOTO+9KUvbfN7Nm3aFHV1dU1eAAAAkLo2C/o1a9ZEfX19dOvWrcl6t27dYtWqVS1uM3To0HjggQdixIgR0bFjx9hvv/3ic5/7XNx9993b/J4JEyZEPp9vfPXo0aOofwcAAAC0hTa/KV4ul2vyvlAoNFvb6tVXX41Ro0bFtddeG/PmzYsnn3wy3nzzzbjiiiu2uf9x48ZFbW1t42v58uVFnR8AAADawm5t9cX77rtvtG/fvtnR+NWrVzc7ar/VhAkT4sQTT4wf/OAHERFx1FFHRadOnWLYsGFxww03RPfu3ZttU1ZWFmVlZcX/AwAAAKANtdkR+o4dO8agQYOiqqqqyXpVVVUMHTq0xW0++OCDaNeu6cjt27ePiI+P7AMAAMD/FW16yv3YsWPj/vvvj6lTp8aSJUtizJgxUV1d3XgK/bhx42LkyJGNnz/nnHPikUceiUmTJsWyZcvihRdeiFGjRsVxxx0X+++/f1v9GQAAALDTtdkp9xERI0aMiLVr18b48eOjpqYmjjzyyJg1a1b07NkzIiJqamqaPJP+4osvjg0bNsQ999wTV155ZXzuc5+LU089NX72s5+11Z8AAAAAbSJX+D92rnpdXV3k8/mora2NioqKth4HAACAz7hSdWib3+UeAAAAyE7QAwAAQIIEPQAAACRI0AMAAECCBD0AAAAkSNADAABAggQ9AAAAJEjQAwAAQIIEPQAAACRI0AMAAECCBD0AAAAkSNADAABAggQ9AAAAJEjQAwAAQIIEPQAAACRI0AMAAECCBD0AAAAkSNADAABAggQ9AAAAJEjQAwAAQIIEPQAAACRI0AMAAECCBD0AAAAkSNADAABAggQ9AAAAJEjQAwAAQIIEPQAAACRI0AMAAECCBD0AAAAkSNADAABAggQ9AAAAJEjQAwAAQIIEPQAAACRI0AMAAECCBD0AAAAkSNADAABAggQ9AAAAJEjQAwAAQIIEPQAAACRI0AMAAECCBD0AAAAkSNADAABAggQ9AAAAJEjQAwAAQIIEPQAAACRI0AMAAECCBD0AAAAkSNADAABAggQ9AAAAJEjQAwAAQIIEPQAAACRI0AMAAECCBD0AAAAkSNADAABAggQ9AAAAJEjQAwAAQIIEPQAAACRI0AMAAECCBD0AAAAkSNADAABAggQ9AAAAJEjQAwAAQIIEPQAAACRI0AMAAECCBD0AAAAkSNADAABAggQ9AAAAJEjQAwAAQIIEPQAAACRI0AMAAECCBD0AAAAkSNADAABAggQ9AAAAJEjQAwAAQIIEPQAAACRI0AMAAECCBD0AAAAkSNADAABAggQ9AAAAJEjQAwAAQIIEPQAAACRI0AMAAECCBD0AAAAkaLesG2zcuDFuuummePrpp2P16tXR0NDQ5PfLli0r2nAAAABAyzIH/WWXXRa/+93v4sILL4zu3btHLpcrxVwAAADAdmQO+ieeeCL+53/+J0488cRSzAMAAAC0QuZr6Pfaa6/Ye++9SzELAAAA0EqZg/7666+Pa6+9Nj744INSzAMAAAC0QuZT7m+77bZ44403olu3btGrV6/o0KFDk9/Pnz+/aMMBAAAALcsc9MOHDy/FHAAAAEAGmYK+vr4+vvjFL8ZRRx0Ve+21V6lmAgAAAD5Fpmvo27dvH2eeeWasX7++VPMAAAAArZD5pnif//znY9myZaWYBQAAAGilzEF/4403xlVXXRW//vWvo6amJurq6pq8AAAAgNLLFQqFQpYN2rX7//8DyOVyjT8XCoXI5XJRX19fvOlKoK6uLvL5fNTW1kZFRUVbjwMAAMBnXKk6NPNd7p999tmifTkAAACwYzIH/cknn1yKOQAAAIAMMgf9nDlztvv7L3zhCzs8DAAAANA6mYP+i1/8YrO1v72Wfle/hh4AAAA+CzLf5f4vf/lLk9fq1avjySefjGOPPTZmz55dihkBAACAT8h8hD6fzzdbO+OMM6KsrCzGjBkT8+bNK8pgAAAAwLZlPkK/LV26dImlS5cWa3cAAADAdmQ+Qr9o0aIm7wuFQtTU1MRNN90URx99dNEGAwAAALYtc9APGDAgcrlcFAqFJusnnHBCTJ06tWiDAQAAANuWOejffPPNJu/btWsXXbp0ifLy8qINBQAAAGxf5mvof/e738V+++0XPXv2jJ49e0aPHj2ivLw8Nm/eHNOnTy/FjAAAAMAn5AqfPHf+U7Rv3z5qamqia9euTdbXrl0bXbt23eWfQ19XVxf5fD5qa2ujoqKirccBAADgM65UHZr5CH2hUIhcLtdsfcWKFS0+0g4AAAAovlZfQ3/MMcdELpeLXC4Xp512Wuy22//ftL6+Pt588834h3/4h5IMCQAAADTV6qAfPnx4REQsXLgwzjzzzNhzzz0bf9exY8fo1atXfPWrXy3+hAAAAEAzrQ766667LiIievXqFSNGjHBXewAAAGhDma+hv+iii+Kjjz6K+++/P8aNGxfr1q2LiIj58+fHypUriz4gAAAA0FzmoF+0aFEceuih8bOf/SxuvfXWWL9+fUREPProozFu3LjMA0ycODF69+4d5eXlMWjQoHjuuee2+/lNmzbFNddcEz179oyysrI4+OCDY+rUqZm/FwAAAFKWOejHjBkTF198cbz++utNTrs/66yzYs6cOZn2NWPGjBg9enRcc801sWDBghg2bFicddZZUV1dvc1tzj///Hj66adjypQpsXTp0njooYfisMMOy/pnAAAAQNIyP4c+n8/H/Pnz4+CDD47OnTvHn/70p+jTp0+8/fbb0a9fv/joo49ava/jjz8+Bg4cGJMmTWpcO/zww2P48OExYcKEZp9/8skn44ILLohly5bF3nvvnWXsRp5DDwAAwM60yzyHvry8POrq6pqtL126NLp06dLq/WzevDnmzZsXlZWVTdYrKyvjxRdfbHGbxx9/PAYPHhw333xzHHDAAXHooYfGVVddFR9++OE2v2fTpk1RV1fX5AUAAACpyxz05557bowfPz62bNkSERG5XC6qq6vj6quvzvTYujVr1kR9fX1069atyXq3bt1i1apVLW6zbNmyeP7552Px4sXx6KOPxh133BEzZ86M73znO9v8ngkTJkQ+n2989ejRo9UzAgAAwK4qc9Dfeuut8d5770XXrl3jww8/jJNPPjn69u0bnTt3jhtvvDHzALlcrsn7QqHQbG2rhoaGyOVy8cADD8Rxxx0XZ599dtx+++0xbdq0bR6lHzduXNTW1ja+li9fnnlGAAAA2NW0+jn0W1VUVMTzzz8fzzzzTMyfPz8aGhpi4MCBcfrpp2faz7777hvt27dvdjR+9erVzY7ab9W9e/c44IADIp/PN64dfvjhUSgUYsWKFXHIIYc026asrCzKysoyzQYAAAC7ukxH6Lds2RKnnHJKvPbaa3HqqafGVVddFT/84Q8zx3xERMeOHWPQoEFRVVXVZL2qqiqGDh3a4jYnnnhivPPOO/H+++83rr322mvRrl27OPDAAzPPAAAAAKnKFPQdOnSIxYsXb/OU+KzGjh0b999/f0ydOjWWLFkSY8aMierq6rjiiisi4uPT5UeOHNn4+a997Wuxzz77xCWXXBKvvvpqzJkzJ37wgx/EN7/5zdh9992LMhMAAACkIPM19CNHjowpU6YU5ctHjBgRd9xxR4wfPz4GDBgQc+bMiVmzZkXPnj0jIqKmpqbJM+n33HPPqKqqivXr18fgwYPj61//epxzzjlx1113FWUeAAAASEXm59B/73vfi+nTp0ffvn1j8ODB0alTpya/v/3224s6YLF5Dj0AAAA7U6k6NPNN8RYvXhwDBw6MiI+vX/9bxToVHwAAANi+zEH/7LPPlmIOAAAAIIPM19ADAAAAbU/QAwAAQIIEPQAAACRI0AMAAECCBD0AAAAkqFV3uX/88cdbvcMvf/nLOzwMAAAA0DqtCvrhw4c3eZ/L5aJQKDR5v1V9fX2RRgMAAAC2pVWn3Dc0NDS+Zs+eHQMGDIgnnngi1q9fH7W1tTFr1qwYOHBgPPnkk6WeFwAAAIhWHqH/W6NHj4577703TjrppMa1M888M/bYY4/453/+51iyZElRBwQAAACay3xTvDfeeCPy+Xyz9Xw+H2+99VYxZgIAAAA+ReagP/bYY2P06NFRU1PTuLZq1aq48sor47jjjivqcAAAAEDLMgf91KlTY/Xq1dGzZ8/o27dv9O3bNw466KCoqamJKVOmlGJGAAAA4BMyX0Pft2/fWLRoUVRVVcWf//znKBQK0b9//zj99NOb3O0eAAAAKJ1c4W+fP5fRRx99FGVlZUmFfF1dXeTz+aitrY2Kioq2HgcAAIDPuFJ1aOZT7hsaGuL666+PAw44IPbcc8948803IyLixz/+sVPuAQAAYCfJHPQ33HBDTJs2LW6++ebo2LFj4/rnP//5uP/++4s6HAAAANCyzEE/ffr0mDx5cnz961+P9u3bN64fddRR8ec//7mowwEAAAAtyxz0K1eujL59+zZbb2hoiC1bthRlKAAAAGD7Mgf9EUccEc8991yz9YcffjiOOeaYogwFAAAAbF/mx9Zdd911ceGFF8bKlSujoaEhHnnkkVi6dGlMnz49fv3rX5diRgAAAOATMh+hP+ecc2LGjBkxa9asyOVyce2118aSJUviv//7v+OMM84oxYwAAADAJ/xdz6FPkefQAwAAsDPtMs+h79OnT6xdu7bZ+vr166NPnz5FGQoAAADYvsxB/9Zbb0V9fX2z9U2bNsXKlSuLMhQAAACwfa2+Kd7jjz/e+PNTTz0V+Xy+8X19fX08/fTT0atXr6IOBwAAALSs1UE/fPjwiIjI5XJx0UUXNfldhw4dolevXnHbbbcVdzoAAACgRa0O+oaGhoiI6N27d/zxj3+Mfffdt2RDAQAAANuX+Tn0b775ZinmAAAAADLIHPQREU8//XQ8/fTTsXr16sYj91tNnTq1KIMBAAAA25Y56H/605/G+PHjY/DgwdG9e/fI5XKlmAsAAADYjsxBf++998a0adPiwgsvLMU8AAAAQCtkfg795s2bY+jQoaWYBQAAAGilzEF/2WWXxYMPPliKWQAAAIBWynzK/UcffRSTJ0+O3/zmN3HUUUdFhw4dmvz+9ttvL9pwAAAAQMsyB/2iRYtiwIABERGxePHiJr9zgzwAAADYOTIH/bPPPluKOQAAAIAMMl9DDwAAALS9zEfoTznllO2eWv/MM8/8XQMBAAAAny5z0G+9fn6rLVu2xMKFC2Px4sVx0UUXFW0wAAAAYNsyB/2//du/tbj+k5/8JN5///2/eyAAAADg0xXtGvpvfOMbMXXq1GLtDgAAANiOogX9Sy+9FOXl5cXaHQAAALAdmU+5P++885q8LxQKUVNTE3Pnzo0f//jHRRsMAAAA2LbMQZ/P55u8b9euXfTr1y/Gjx8flZWVRRsMAAAA2LbMQf+LX/yiFHMAAAAAGWQO+q3mzZsXS5YsiVwuF/37949jjjmmmHMBAAAA25E56FevXh0XXHBB/Pa3v43Pfe5zUSgUora2Nk455ZT45S9/GV26dCnFnAAAAMDfyHyX++9973tRV1cXr7zySqxbty7+8pe/xOLFi6Ouri5GjRpVihkBAACAT8gVCoVClg3y+Xz85je/iWOPPbbJ+h/+8IeorKyM9evXF3XAYqurq4t8Ph+1tbVRUVHR1uMAAADwGVeqDs18hL6hoSE6dOjQbL1Dhw7R0NBQlKEAAACA7csc9Keeemp8//vfj3feeadxbeXKlTFmzJg47bTTijocAAAA0LLMQX/PPffEhg0bolevXnHwwQdH3759o3fv3rFhw4a4++67SzEjAAAA8AmZ73Lfo0ePmD9/flRVVcWf//znKBQK0b9//zj99NNLMR8AAADQgsw3xUudm+IBAACwM+0yN8UbNWpU3HXXXc3W77nnnhg9enRRhgIAAAC2L3PQ/+pXv4oTTzyx2frQoUNj5syZRRkKAAAA2L7MQb927drI5/PN1isqKmLNmjVFGQoAAADYvsxB37dv33jyySebrT/xxBPRp0+fogwFAAAAbF/mu9yPHTs2vvvd78Z7770Xp556akREPP3003HbbbfFHXfcUfQBAQAAgOYyB/03v/nN2LRpU9x4441x/fXXR0REr169YtKkSTFy5MiiDwgAAAA093c9tu69996L3XffPfbcc89izlRSHlsHAADAzrTLPLbuww8/jA8++CAiIrp06RJr166NO+64I2bPnl20oQAAAIDtyxz05557bkyfPj0iItavXx/HHXdc3HbbbXHuuefGpEmTij4gAAAA0FzmoJ8/f34MGzYsIiJmzpwZ++23X7z99tsxffr0uOuuu4o+IAAAANBc5qD/4IMPonPnzhERMXv27DjvvPOiXbt2ccIJJ8Tbb79d9AEBAACA5nboOfSPPfZYLF++PJ566qmorKyMiIjVq1e7yRwAAADsJJmD/tprr42rrroqevXqFccff3wMGTIkIj4+Wn/MMccUfUAAAACguR16bN2qVauipqYmjj766GjX7uP/CfzhD3+IioqKOOyww4o+ZDF5bB0AAAA7U6k6dLcd2Wi//faL/fbbr8nacccdV5SBAAAAgE/XqqA/77zzYtq0aVFRURHnnXfedj/7yCOPFGUwAAAAYNtaFfT5fD5yuVzjzwAAAEDb2qFr6FPmGnoAAAB2plJ1aOa73AMAAABtr1Wn3B9zzDGNp9x/mvnz5/9dAwEAAACfrlVBP3z48MafP/roo5g4cWL079+/8Rn0v//97+OVV16Jb3/726WZEgAAAGiiVUF/3XXXNf582WWXxahRo+L6669v9pnly5cXdzoAAACgRZlvipfP52Pu3LlxyCGHNFl//fXXY/DgwVFbW1vUAYvNTfEAAADYmXaZm+Ltvvvu8fzzzzdbf/7556O8vLwoQwEAAADb16pT7v/W6NGj41vf+lbMmzcvTjjhhIj4+Br6qVOnxrXXXlv0AQEAAIDmMgf91VdfHX369Ik777wzHnzwwYiIOPzww2PatGlx/vnnF31AAAAAoLnM19CnzjX0AAAA7Ey7zDX0AAAAQNsT9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACWrVY+vGjh3b6h3efvvtOzwMAAAA0DqtCvoFCxY0eT9v3ryor6+Pfv36RUTEa6+9Fu3bt49BgwYVf0IAAACgmVYF/bPPPtv48+233x6dO3eO//iP/4i99torIiL+8pe/xCWXXBLDhg0rzZQAAABAE7lCoVDIssEBBxwQs2fPjiOOOKLJ+uLFi6OysjLeeeedog5YbHV1dZHP56O2tjYqKiraehwAAAA+40rVoZlvildXVxfvvvtus/XVq1fHhg0bijIUAAAAsH2Zg/4rX/lKXHLJJTFz5sxYsWJFrFixImbOnBmXXnppnHfeeaWYEQAAAPiEVl1D/7fuvffeuOqqq+Ib3/hGbNmy5eOd7LZbXHrppXHLLbcUfUAAAACguczX0G+1cePGeOONN6JQKETfvn2jU6dOxZ6tJFxDDwAAwM60y1xDv1VNTU3U1NTEoYceGp06dYod/L8AAAAAsAMyB/3atWvjtNNOi0MPPTTOPvvsqKmpiYiIyy67LK688sqiDwgAAAA0lznox4wZEx06dIjq6urYY489GtdHjBgRTz75ZFGHAwAAAFqW+aZ4s2fPjqeeeioOPPDAJuuHHHJIvP3220UbDAAAANi2zEfoN27c2OTI/FZr1qyJsrKyogwFAAAAbF/moP/CF74Q06dPb3yfy+WioaEhbrnlljjllFOKOhwAAADQssyn3N9yyy3xxS9+MebOnRubN2+OH/7wh/HKK6/EunXr4oUXXijFjAAAAMAnZD5C379//1i0aFEcd9xxccYZZ8TGjRvjvPPOiwULFsTBBx9cihkBAACAT8gV/o89QL6uri7y+XzU1tZGRUVFW48DAADAZ1ypOjTzKfeLFi1qcT2Xy0V5eXkcdNBBbo4HAAAAJZY56AcMGBC5XC4iIrYe3N/6PiKiQ4cOMWLEiLjvvvuivLy8SGMCAAAAfyvzNfSPPvpoHHLIITF58uT405/+FAsXLozJkydHv3794sEHH4wpU6bEM888Ez/60Y9KMS8AAAAQO3CE/sYbb4w777wzzjzzzMa1o446Kg488MD48Y9/HH/4wx+iU6dOceWVV8att95a1GEBAACAj2U+Qv/yyy9Hz549m6337NkzXn755Yj4+LT8mpqav386AAAAoEWZg/6www6Lm266KTZv3ty4tmXLlrjpppvisMMOi4iIlStXRrdu3Yo3JQAAANBE5lPuf/7zn8eXv/zlOPDAA+Ooo46KXC4XixYtivr6+vj1r38dERHLli2Lb3/720UfFgAAAPjYDj2H/v3334//+q//itdeey0KhUIcdthh8bWvfS06d+5cihmLynPoAQAA2Jl2mefQR0TsueeeccUVVxRlgIkTJ8Ytt9wSNTU1ccQRR8Qdd9wRw4YN+9TtXnjhhTj55JPjyCOPjIULFxZlFgAAAEjFDgV9RMSrr74a1dXVTa6lj4j48pe/3Op9zJgxIyM+6psAAB9kSURBVEaPHh0TJ06ME088Me67774466yz4tVXX42DDjpom9vV1tbGyJEj47TTTot33313R/8EAAAASFbmU+6XLVsWX/nKV+Lll1+OXC4XWzfP5XIREVFfX9/qfR1//PExcODAmDRpUuPa4YcfHsOHD48JEyZsc7sLLrggDjnkkGjfvn089thj2z1Cv2nTpti0aVPj+7q6uujRo4dT7gEAANgpSnXKfea73H//+9+P3r17x7vvvht77LFHvPLKKzFnzpwYPHhw/Pa3v231fjZv3hzz5s2LysrKJuuVlZXx4osvbnO7X/ziF/HGG2/Edddd16rvmTBhQuTz+cZXjx49Wj0jAAAA7KoyB/1LL70U48ePjy5dukS7du2iXbt2cdJJJ8WECRNi1KhRrd7PmjVror6+vtnj7bp16xarVq1qcZvXX389rr766njggQdit91ad7XAuHHjora2tvG1fPnyVs8IAAAAu6rM19DX19fHnnvuGRER++67b7zzzjvRr1+/6NmzZyxdujTzAFtP1d+qUCg0W9v6vV/72tfipz/9aRx66KGt3n9ZWVmUlZVlngsAAAB2ZZmD/sgjj4xFixZFnz594vjjj4+bb745OnbsGJMnT44+ffq0ej/77rtvtG/fvtnR+NWrVzc7ah8RsWHDhpg7d24sWLAgvvvd70ZERENDQxQKhdhtt91i9uzZceqpp2b9cwAAACBJmYP+Rz/6UWzcuDEiIm644Yb4x3/8xxg2bFjss88+MWPGjFbvp2PHjjFo0KCoqqqKr3zlK43rVVVVce655zb7fEVFRbz88stN1iZOnBjPPPNMzJw5M3r37p31TwEAAIBkZQ76M888s/HnPn36xKuvvhrr1q2Lvfbaq8VT5bdn7NixceGFF8bgwYNjyJAhMXny5Kiurm58xv24ceNi5cqVMX369GjXrl0ceeSRTbbv2rVrlJeXN1sHAACAz7pMQf/Xv/41ysvLY+HChU0ieu+9996hLx8xYkSsXbs2xo8fHzU1NXHkkUfGrFmzomfPnhERUVNTE9XV1Tu0bwAAAPgsy/wc+oMPPjgeeeSROProo0s1U0mV6vl/AAAA0JJd5jn0P/rRj2LcuHGxbt26og0BAAAAZJP5Gvq77ror/vd//zf233//6NmzZ3Tq1KnJ7+fPn1+04QAAAICWZQ764cOHl2IOAAAAIIPM19CnzjX0AAAA7Ey7zDX0ERHr16+P+++/v8m19PPnz4+VK1cWbTAAAABg2zKfcr9o0aI4/fTTI5/Px1tvvRX/9E//FHvvvXc8+uij8fbbb8f06dNLMScAAADwNzIfoR87dmxcfPHF8frrr0d5eXnj+llnnRVz5swp6nAAAABAyzIH/R//+Me4/PLLm60fcMABsWrVqqIMBQAAAGxf5qAvLy+Purq6ZutLly6NLl26FGUoAAAAYPsyB/25554b48ePjy1btkRERC6Xi+rq6rj66qvjq1/9atEHBAAAAJrLHPS33nprvPfee9G1a9f48MMP4+STT46+fftG586d48YbbyzFjAAAAMAnZL7LfUVFRTz//PPxzDPPxPz586OhoSEGDhwYp59+einmAwAAAFqQKxQKhSwbvPXWW9GrV68SjVN6dXV1kc/no7a2NioqKtp6HAAAAD7jStWhmU+579OnT5x00klx3333xbp164o2CAAAANB6mYN+7ty5MWTIkLjhhhti//33j3PPPTcefvjh2LRpUynmAwAAAFqQOegHDhwYt9xyS1RXV8cTTzwRXbt2jcsvvzy6du0a3/zmN0sxIwAAAPAJma+hb8n8+fPj0ksvjUWLFkV9fX0x5ioZ19ADAACwM+0y19BvtXz58rj55ptjwIABceyxx0anTp3innvuKdpgAAAAwLZlfmzd5MmT44EHHogXXngh+vXrF1//+tfjscceS/rO9wAAAJCazEF//fXXxwUXXBB33nlnDBgwoBQzAQAAAJ8ic9BXV1dHLpdr8XcLFy4U+QAAALATZL6G/pMxX1tbGxMnToyBAwfGoEGDijYYAAAAsG07fFO8Z555Jr7xjW9E9+7d4+67746zzz475s6dW8zZAAAAgG3IdMr9ihUrYtq0aTF16tTYuHFjnH/++bFly5b41a9+Ff379y/VjAAAAMAntPoI/dlnnx39+/ePV199Ne6+++5455134u677y7lbAAAAMA2tPoI/ezZs2PUqFHxrW99Kw455JBSzgQAAAB8ilYfoX/uuediw4YNMXjw4Dj++OPjnnvuiffee6+UswEAAADb0OqgHzJkSPz7v/971NTUxOWXXx6//OUv44ADDoiGhoaoqqqKDRs2lHJOAAAA4G/kCoVCYUc3Xrp0aUyZMiX+8z//M9avXx9nnHFGPP7448Wcr+jq6uoin89HbW1tVFRUtPU4AAAAfMaVqkN3+LF1ERH9+vWLm2++OVasWBEPPfRQsWYCAAAAPsXfdYQ+RY7QAwAAsDPtkkfoAQAAgLYh6AEAACBBgh4AAAASJOgBAAAgQYIeAAAAEiToAQAAIEGCHgAAABIk6AEAACBBgh4AAAASJOgBAAAgQYIeAAAAEiToAQAAIEGCHgAAABIk6AEAACBBgh4AAAASJOgBAAAgQYIeAAAAEiToAQAAIEGCHgAAABIk6AEAACBBgh4AAAASJOgBAAAgQYIeAAAAEiToAQAAIEGCHgAAABIk6AEAACBBgh4AAAASJOgBAAAgQYIeAAAAEiToAQAAIEGCHgAAABIk6AEAACBBgh4AAAASJOgBAAAgQYIeAAAAEiToAQAAIEGCHgAAABIk6AEAACBBgh4AAAASJOgBAAAgQYIeAAAAEiToAQAAIEGCHgAAABIk6AEAACBBgh4AAAASJOgBAAAgQYIeAAAAEiToAQAAIEGCHgAAABIk6AEAACBBgh4AAAASJOgBAAAgQYIeAAAAEiToAQAAIEGCHgAAABIk6AEAACBBgh4AAAASJOgBAAAgQYIeAAAAEiToAQAAIEGCHgAAABIk6AEAACBBgh4AAAASJOgBAAAgQYIeAAAAEiToAQAAIEGCHgAAABIk6AEAACBBgh4AAAASJOgBAAAgQYIeAAAAEiToAQAAIEGCHgAAABIk6AEAACBBgh4AAAASJOgBAAAgQYIeAAAAEiToAQAAIEGCHgAAABIk6AEAACBBgh4AAAAS1OZBP3HixOjdu3eUl5fHoEGD4rnnntvmZx955JE444wzokuXLlFRURFDhgyJp556aidOCwAAALuGNg36GTNmxOjRo+Oaa66JBQsWxLBhw+Kss86K6urqFj8/Z86cOOOMM2LWrFkxb968OOWUU+Kcc86JBQsW7OTJAQAAoG3lCoVCoa2+/Pjjj4+BAwfGpEmTGtcOP/zwGD58eEyYMKFV+zjiiCNixIgRce2117bq83V1dZHP56O2tjYqKip2aG4AAABorVJ1aJsdod+8eXPMmzcvKisrm6xXVlbGiy++2Kp9NDQ0xIYNG2Lvvffe5mc2bdoUdXV1TV4AAACQujYL+jVr1kR9fX1069atyXq3bt1i1apVrdrHbbfdFhs3bozzzz9/m5+ZMGFC5PP5xlePHj3+rrkBAABgV9DmN8XL5XJN3hcKhWZrLXnooYfiJz/5ScyYMSO6du26zc+NGzcuamtrG1/Lly//u2cGAACAtrZbW33xvvvuG+3bt292NH716tXNjtp/0owZM+LSSy+Nhx9+OE4//fTtfrasrCzKysr+7nkBAABgV9JmR+g7duwYgwYNiqqqqibrVVVVMXTo0G1u99BDD8XFF18cDz74YHzpS18q9ZgAAACwS2qzI/QREWPHjo0LL7wwBg8eHEOGDInJkydHdXV1XHHFFRHx8enyK1eujOnTp0fExzE/cuTIuPPOO+OEE05oPLq/++67Rz6fb7O/AwAAAHa2Ng36ESNGxNq1a2P8+PFRU1MTRx55ZMyaNSt69uwZERE1NTVNnkl/3333xV//+tf4zne+E9/5znca1y+66KKYNm3azh4fAAAA2kybPoe+LXgOPQAAADvTZ+459AAAAMCOE/QAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEPAAAACRL0AAAAkCBBDwAAAAkS9AAAAJAgQQ8AAAAJEvQAAACQIEEP/L/27j0oqvr/4/hrZYEFE0pEJDAVw7iUiJAGxqgpOtpoVo5lllraDF3GQca8FmJSlJk5XkAztClvjGbmlKl0ATVtSmZxmmC6gIpOklGGd7yd7x8O+/sRlO4Kuyw8HzM77n74nD3vc+Y9Ky8+e3YBAAAAuCECPQAAAAAAbohADwAAAACAGyLQAwAAAADghgj0AAAAAAC4IQI9AAAAAABuiEAPAAAAAIAbItADAAAAAOCGCPQAAAAAALghAj0AAAAAAG6IQA8AAAAAgBsi0AMAAAAA4IYI9AAAAAAAuCECPQAAAAAAbsjlgT47O1vdunWTxWJRXFyc9uzZ85/zCwsLFRcXJ4vForCwMK1YscJJlQIAAAAA0Hy4NNDn5eUpNTVVc+bMkdVqVVJSkoYNG6aKiooG5x86dEjDhw9XUlKSrFarZs+erSlTpuijjz5ycuUAAAAAALiWyTAMw1U779u3r3r37q2cnBzbWGRkpEaNGqWsrKx682fMmKFt27aptLTUNpaSkqKDBw9q//79N7TPU6dOyd/fX9XV1fLz87v5gwAAAAAA4D80VQ41N9oz2enixYsqKirSzJkz64wPGTJE+/bta3Cb/fv3a8iQIXXGhg4dqtzcXF26dEmenp71tqmpqVFNTY3tcXV1taRrJxQAAAAAgKZWmz8bez3dZYG+qqpKV65cUVBQUJ3xoKAgVVZWNrhNZWVlg/MvX76sqqoqBQcH19smKytL8+bNqzfeuXPnm6geAAAAAAD7/Pnnn/L392+053NZoK9lMpnqPDYMo97Y9eY3NF5r1qxZSktLsz3++++/1aVLF1VUVDTqiQSak1OnTqlz5846evQol5agxaLP0RrQ52gN6HO0BtXV1brjjjvUvn37Rn1elwX6Dh06yMPDo95q/IkTJ+qtwtfq1KlTg/PNZrMCAgIa3Mbb21ve3t71xv39/XnBQIvn5+dHn6PFo8/RGtDnaA3oc7QGbdo07ufSu+xT7r28vBQXF6f8/Pw64/n5+UpMTGxwm4SEhHrzd+3apfj4+AavnwcAAAAAoKVy6dfWpaWl6b333tPq1atVWlqqqVOnqqKiQikpKZKuvV1+/PjxtvkpKSk6cuSI0tLSVFpaqtWrVys3N1fTpk1z1SEAAAAAAOASHhkZGRmu2vndd9+tgIAAvf7661q4cKHOnz+vDz/8UDExMZKktWvX6siRI5o4caIk6bbbbtP999+vlStXav78+bJarXrttdfqhP4b4eHhoQEDBshsdvlHCABNhj5Ha0CfozWgz9Ea0OdoDZqiz136PfQAAAAAAMAxLn3LPQAAAAAAcAyBHgAAAAAAN0SgBwAAAADADRHoAQAAAABwQy0y0GdnZ6tbt26yWCyKi4vTnj17/nN+YWGh4uLiZLFYFBYWphUrVjipUsBx9vT5li1blJycrMDAQPn5+SkhIUE7d+50YrWAY+x9Pa/1zTffyGw2q1evXk1cIXDz7O3zmpoazZkzR126dJG3t7e6d++u1atXO6lawDH29vm6desUExMjX19fBQcH6+mnn9aff/7ppGoB++3evVsjRozQ7bffLpPJpK1bt153m8bIoS0u0Ofl5Sk1NVVz5syR1WpVUlKShg0bpoqKigbnHzp0SMOHD1dSUpKsVqtmz56tKVOm6KOPPnJy5cCNs7fPd+/ereTkZG3fvl1FRUUaOHCgRowYIavV6uTKgRtnb5/Xqq6u1vjx4zVo0CAnVQo4zpE+HzNmjL788kvl5ubqp59+0oYNGxQREeHEqgH72Nvne/fu1fjx4zVp0iT9+OOP2rRpk77//ntNnjzZyZUDN+7s2bOKiYnRsmXLbmh+Y+XQFve1dX379lXv3r2Vk5NjG4uMjNSoUaOUlZVVb/6MGTO0bds2lZaW2sZSUlJ08OBB7d+/3yk1A/ayt88bEh0drccee0zp6elNVSZwUxzt88cff1zh4eHy8PDQ1q1bVVxc7IxyAYfY2+c7duzQ448/rvLycrVv396ZpQIOs7fPFy5cqJycHJWVldnGli5dqgULFujo0aNOqRm4GSaTSR9//LFGjRr1r3MaK4e2qBX6ixcvqqioSEOGDKkzPmTIEO3bt6/Bbfbv319v/tChQ3XgwAFdunSpyWoFHOVIn//T1atXdfr0aX4ZRLPlaJ+vWbNGZWVlmjt3blOXCNw0R/p827Ztio+P14IFCxQSEqIePXpo2rRpOn/+vDNKBuzmSJ8nJibq2LFj2r59uwzD0O+//67NmzfrwQcfdEbJgFM0Vg41N3ZhrlRVVaUrV64oKCioznhQUJAqKysb3KaysrLB+ZcvX1ZVVZWCg4ObrF7AEY70+T+9/fbbOnv2rMaMGdMUJQI3zZE+/+WXXzRz5kzt2bNHZnOL+u8NLZQjfV5eXq69e/fKYrHo448/VlVVlZ5//nn99ddfXEePZsmRPk9MTNS6dev02GOP6cKFC7p8+bJGjhyppUuXOqNkwCkaK4e2qBX6WiaTqc5jwzDqjV1vfkPjQHNib5/X2rBhgzIyMpSXl6eOHTs2VXlAo7jRPr9y5YqeeOIJzZs3Tz169HBWeUCjsOf1/OrVqzKZTFq3bp369Omj4cOHa9GiRXr//fdZpUezZk+fl5SUaMqUKUpPT1dRUZF27NihQ4cOKSUlxRmlAk7TGDm0RS1hdOjQQR4eHvX+2nfixIl6f/2o1alTpwbnm81mBQQENFmtgKMc6fNaeXl5mjRpkjZt2qTBgwc3ZZnATbG3z0+fPq0DBw7IarXqxRdflHQt+BiGIbPZrF27dumBBx5wSu3AjXLk9Tw4OFghISHy9/e3jUVGRsowDB07dkzh4eFNWjNgL0f6PCsrS/369dNLL70kSerZs6fatm2rpKQkZWZm8g5atAiNlUNb1Aq9l5eX4uLilJ+fX2c8Pz9fiYmJDW6TkJBQb/6uXbsUHx8vT0/PJqsVcJQjfS5dW5mfOHGi1q9fzzVoaPbs7XM/Pz/98MMPKi4utt1SUlJ01113qbi4WH379nVW6cANc+T1vF+/fvrtt9905swZ29jPP/+sNm3aKDQ0tEnrBRzhSJ+fO3dObdrUjSkeHh6S/m8FE3B3jZZDjRZm48aNhqenp5Gbm2uUlJQYqampRtu2bY3Dhw8bhmEYM2fONJ566inb/PLycsPX19eYOnWqUVJSYuTm5hqenp7G5s2bXXUIwHXZ2+fr1683zGazsXz5cuP48eO2299//+2qQwCuy94+/6e5c+caMTExzioXcIi9fX769GkjNDTUGD16tPHjjz8ahYWFRnh4uDF58mRXHQJwXfb2+Zo1awyz2WxkZ2cbZWVlxt69e434+HijT58+rjoE4LpOnz5tWK1Ww2q1GpKMRYsWGVar1Thy5IhhGE2XQ1tcoDcMw1i+fLnRpUsXw8vLy+jdu7dRWFho+9mECROM/v3715lfUFBgxMbGGl5eXkbXrl2NnJwcJ1cM2M+ePu/fv78hqd5twoQJzi8csIO9r+f/H4Ee7sLePi8tLTUGDx5s+Pj4GKGhoUZaWppx7tw5J1cN2MfePl+yZIkRFRVl+Pj4GMHBwca4ceOMY8eOOblq4MZ9/fXX//n7dlPl0Bb3PfQAAAAAALQGLeoaegAAAAAAWgsCPQAAAAAAbohADwAAAACAGyLQAwAAAADghgj0AAAAAAC4IQI9AAAAAABuiEAPAAAAAIAbItADAAAAAOCGCPQAALipw4cPy2Qyqbi42NWlAAAAFyDQAwDQDJlMpv+8TZw4UZ07d9bx48d19913O72+8vJyjR07VrfffrssFotCQ0P10EMP6eeff5bEHxsAAHAGs6sLAAAA9R0/ftx2Py8vT+np6frpp59sYz4+PvLw8FCnTp2cXtvFixeVnJysiIgIbdmyRcHBwTp27Ji2b9+u6upqp9cDAEBrxQo9AADNUKdOnWw3f39/mUymemP/XAUvKCiQyWTSzp07FRsbKx8fHz3wwAM6ceKEPv/8c0VGRsrPz09jx47VuXPnbPsyDEMLFixQWFiYfHx8FBMTo82bN/9rbSUlJSovL1d2drbuu+8+denSRf369dNrr72me++9V5LUrVs3SVJsbKxMJpMGDBhg237NmjWKjIyUxWJRRESEsrOzbT+rPaaNGzcqMTFRFotF0dHRKigosM05efKkxo0bp8DAQPn4+Cg8PFxr1qxpjNMOAIBbYYUeAIAWJiMjQ8uWLZOvr6/GjBmjMWPGyNvbW+vXr9eZM2f08MMPa+nSpZoxY4Yk6eWXX9aWLVuUk5Oj8PBw7d69W08++aQCAwPVv3//es8fGBioNm3aaPPmzUpNTZWHh0e9Od9995369OmjL774QtHR0fLy8pIkrVq1SnPnztWyZcsUGxsrq9WqZ599Vm3bttWECRNs27/00ktavHixoqKitGjRIo0cOVKHDh1SQECAXnnlFZWUlOjzzz9Xhw4d9Ouvv+r8+fNNdDYBAGi+CPQAALQwmZmZ6tevnyRp0qRJmjVrlsrKyhQWFiZJGj16tL7++mvNmDFDZ8+e1aJFi/TVV18pISFBkhQWFqa9e/dq5cqVDQb6kJAQLVmyRNOnT9e8efMUHx+vgQMHaty4cbZ9BAYGSpICAgLqXBYwf/58vf3223rkkUckXVvJLykp0cqVK+sE+hdffFGPPvqoJCknJ0c7duxQbm6upk+froqKCsXGxio+Pl6S1LVr18Y8fQAAuA3ecg8AQAvTs2dP2/2goCD5+vragnbt2IkTJyRde/v8hQsXlJycrFtuucV2++CDD1RWVvav+3jhhRdUWVmptWvXKiEhQZs2bVJ0dLTy8/P/dZs//vhDR48e1aRJk+rsKzMzs96+av+4IElms1nx8fEqLS2VJD333HPauHGjevXqpenTp2vfvn32nSAAAFoIVugBAGhhPD09bfdNJlOdx7VjV69elSTbv5999plCQkLqzPP29v7P/bRr104jR47UyJEjlZmZqaFDhyozM1PJyckNzq/d16pVq9S3b986P2vobfv/ZDKZJEnDhg3TkSNH9Nlnn+mLL77QoEGD9MILL2jhwoXXfQ4AAFoSVugBAGjFoqKi5O3trYqKCt155511bp07d77h5zGZTIqIiNDZs2clyXbN/JUrV2xzgoKCFBISovLy8nr7qv0QvVrffvut7f7ly5dVVFSkiIgI21hgYKAmTpyotWvXavHixXr33XcdOn4AANwZK/QAALRi7dq107Rp0zR16lRdvXpV999/v06dOqV9+/bplltuqXNde63i4mLNnTtXTz31lKKiouTl5aXCwkKtXr3a9kF7HTt2lI+Pj3bs2KHQ0FBZLBb5+/srIyNDU6ZMkZ+fn4YNG6aamhodOHBAJ0+eVFpamm0fy5cvV3h4uCIjI/XOO+/o5MmTeuaZZyRJ6enpiouLU3R0tGpqavTpp58qMjLSOScMAIBmhEAPAEArN3/+fHXs2FFZWVkqLy/Xrbfeqt69e2v27NkNzg8NDVXXrl01b94829fM1T6eOnWqpGvXvS9ZskSvvvqq0tPTlZSUpIKCAk2ePFm+vr566623NH36dLVt21b33HOPUlNT6+zjjTfe0Jtvvimr1aru3bvrk08+UYcOHSRdW/2fNWuWDh8+LB8fHyUlJWnjxo1Ne5IAAGiGTIZhGK4uAgAAQLr2PfTdunWT1WpVr169XF0OAADNGtfQAwAAAADghgj0AAAAAAC4Id5yDwAAAACAG2KFHgAAAAAAN0SgBwAAAADADRHoAQAAAABwQwR6AAAAAADcEIEeAAAAAAA3RKAHAAAAAMANEegBAAAAAHBDBHoAAAAAANzQ/wBsaPv2FziemwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_curves([np.array(results)], [\"semi-gradient SARSA\"], [\"b\"],\n",
    "            \"Averaged discounted return\", \"Q3 - (a): semi-gradient SARSA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3a745f",
   "metadata": {},
   "source": [
    "# Q3 - (b) [5180]: Implement the semi-gradient SARSA with Tile-based/Room-based aggregation.\n",
    "\n",
    "As described in the question, you are asked to implement the semi-gradient SARSA with **Tile-based/Room-based** state aggregation strategy. That is grouping the nearby states in a n x n (i.e., n = 2) tile as one aggregated state and grouping states in one room as one aggregated state. \n",
    "\n",
    "**Plot**: Plot the learning curves of tile size n = 2 and Room-based aggregation in the same plot. You can use the plot function above to generate the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Implement the Tile-based Agent here. We inherit it from the \"SemiGradientSARSAAgent\" above\n",
    "\"\"\"\n",
    "class TileAgent(SemiGradientSARSAAgent):\n",
    "    def __init__(self, env, info):\n",
    "        \"\"\"\n",
    "        Function to initialize the semi-gradient SARSA agent\n",
    "        Args:\n",
    "            env: the environment that the agent interacts with\n",
    "            info (dict): a dictionary variable contains all necessary parameters.\n",
    "\n",
    "        Note that: In this question, we will implement a simple state aggregation strategies.\n",
    "                   Specifically, we design the following function approximation:\n",
    "                   1. Feature: we use the one-hot encoding to compute the feature for each state-action pair.\n",
    "                               E.g., state = [0, 0] and action = \"Up\" (state-action pair) will correspond to\n",
    "                               a unique one-hot representation $f(s, a) = [0, 0, 0, 1, 0, ..., 0]$.\n",
    "                   2. Weights: we define a weight vector $w$ having the sample shape as the feature vector.\n",
    "                               Specifically, the Q(s, a) can be estimated by Q(s, a) = w^{T} * f(s, a)\n",
    "\n",
    "        Importantly, as described in the question, we only aggregate the states.\n",
    "        \"\"\"\n",
    "        super().__init__(env, info)\n",
    "        \n",
    "        \"\"\" State aggregation for semi-gradient SARSA \"\"\"\n",
    "        # Compute the total number of state after the aggregation\n",
    "        self.state_space, self.state_num = self.create_state_aggregation()\n",
    "\n",
    "        # Compute the total number of the actions\n",
    "        self.action_num = len(self.env.action_names)\n",
    "\n",
    "        \"\"\" Function approximation for semi-gradient SARSA\"\"\"\n",
    "        # We create a weight with shape |S| * |A|\n",
    "        self.weights_fn = np.zeros((self.state_num * self.action_num))\n",
    "\n",
    "        # We construct a numpy array that contains the one-hot features for all state-action pairs.\n",
    "        # The size is (|S| * |A|) x (|S| * |A|).\n",
    "        # Each i-th row is the one-hot encoding for state-action pair with index i.\n",
    "        self.feature_arr = np.eye(self.state_num * self.action_num)\n",
    "\n",
    "    def create_state_aggregation(self):\n",
    "        \"\"\"\n",
    "        Function that returns the aggregated state space and the number of the aggregated states.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"CODE HERE: Implement the Tile-based state aggregation here.\n",
    "           Hint: you can manually discretize the original states using the Tile-based method.\n",
    "           For example, you can copy the grid from the Four Rooms environment\n",
    "           and manually aggregate the states (value = 0) in the grid. \n",
    "           \n",
    "           You have to return:\n",
    "            1. the aggregated state space (Any data structure that you find it easier to render the index of the \n",
    "               aggregated state.)\n",
    "            2. the number of the aggregated states (int)\n",
    "        \"\"\"\n",
    "        \n",
    "        # define the aggregated state space using Tile-based method (2x2)\n",
    "        aggregated_state_space = None\n",
    "\n",
    "        # aggregate the state space\n",
    "        aggregate_state_num = None\n",
    "\n",
    "        return aggregated_state_space, aggregate_state_num\n",
    "\n",
    "    def _aggregate_state_idx(self, state):\n",
    "        \"\"\"\n",
    "        Function returns the index of aggregated state given an original state\n",
    "\n",
    "        Args:\n",
    "            state (list): original state\n",
    "        \"\"\"\n",
    "        \"\"\"CODE HERE: based on your state aggregation, return the index of the aggregated state given an original\n",
    "           state. \n",
    "           \n",
    "           You have to return:\n",
    "           1. index (int) of the aggregated state given the original state\n",
    "        \"\"\"\n",
    "        \n",
    "        # render the index of the aggregated state\n",
    "        state_idx = None\n",
    "        return state_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fcaec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Implement the Room-based Agent here. We inherit it from the \"SemiGradientSARSAAgent\" above\n",
    "\"\"\"\n",
    "class RoomAgent(SemiGradientSARSAAgent):\n",
    "    def __init__(self, env, info):\n",
    "        \"\"\"\n",
    "        Function to initialize the semi-gradient SARSA agent\n",
    "        Args:\n",
    "            env: the environment that the agent interacts with\n",
    "            info (dict): a dictionary variable contains all necessary parameters.\n",
    "\n",
    "        Note that: In this question, we will implement a simple state aggregation strategies.\n",
    "                   Specifically, we design the following function approximation:\n",
    "                   1. Feature: we use the one-hot encoding to compute the feature for each state-action pair.\n",
    "                               E.g., state = [0, 0] and action = \"Up\" (state-action pair) will correspond to\n",
    "                               a unique one-hot representation $f(s, a) = [0, 0, 0, 1, 0, ..., 0]$.\n",
    "                   2. Weights: we define a weight vector $w$ having the sample shape as the feature vector.\n",
    "                               Specifically, the Q(s, a) can be estimated by Q(s, a) = w^{T} * f(s, a)\n",
    "\n",
    "        Importantly, as described in the question, we only aggregate the states.\n",
    "        \"\"\"\n",
    "        super().__init__(env, info)\n",
    "        \n",
    "        \"\"\" State aggregation for semi-gradient SARSA \"\"\"\n",
    "        # Compute the total number of state after the aggregation\n",
    "        self.state_space, self.state_num = self.create_state_aggregation()\n",
    "\n",
    "        # Compute the total number of the actions\n",
    "        self.action_num = len(self.env.action_names)\n",
    "\n",
    "        \"\"\" Function approximation for semi-gradient SARSA\"\"\"\n",
    "        # We create a weight with shape |S| * |A|\n",
    "        self.weights_fn = np.zeros((self.state_num * self.action_num))\n",
    "\n",
    "        # We construct a numpy array that contains the one-hot features for all state-action pairs.\n",
    "        # The size is (|S| * |A|) x (|S| * |A|).\n",
    "        # Each i-th row is the one-hot encoding for state-action pair with index i.\n",
    "        self.feature_arr = np.eye(self.state_num * self.action_num)\n",
    "\n",
    "    def create_state_aggregation(self):\n",
    "        \"\"\"\n",
    "        Function that returns the aggregated state space and the number of the aggregated states.\n",
    "        \"\"\"\n",
    "        \"\"\"CODE HERE: your state aggregation strategy. Hint: you can start with a simple state aggregation\n",
    "           that just aggregate each state to itself. In other words, the aggregated state space is just\n",
    "           the original state space.\n",
    "           \n",
    "           You have to return:\n",
    "            1. the aggregated state space (Any data structure that you find it easier to render the index of the \n",
    "               aggregated state.)\n",
    "            2. the number of the aggregated states (int)\n",
    "        \"\"\"\n",
    "\n",
    "        aggregated_state_space = None\n",
    "\n",
    "        # aggregate the state space\n",
    "        aggregate_state_num = None\n",
    "\n",
    "        return aggregated_state_space, aggregate_state_num\n",
    "\n",
    "    def _aggregate_state_idx(self, state):\n",
    "        \"\"\"\n",
    "        Function returns the index of aggregated state given an original state\n",
    "\n",
    "        Args:\n",
    "            state (list): original state\n",
    "        \"\"\"\n",
    "        \"\"\"CODE HERE: based on your state aggregation, return the index of the aggregated state given an original\n",
    "           state. \n",
    "           \n",
    "           You have to return:\n",
    "           1. index (int) of the aggregated state given the original state\n",
    "        \"\"\"\n",
    "        state_idx = None\n",
    "        return state_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad0474",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set the random seed\n",
    "    np.random.seed(1234)\n",
    "    random.seed(1234)\n",
    "\n",
    "    # set hyper-parameters\n",
    "    params = {\n",
    "        \"episode_num\": 100,\n",
    "        \"alpha\": 0.1,\n",
    "        \"gamma\": 0.99,\n",
    "        \"epsilon\": 0.1\n",
    "    }\n",
    "\n",
    "    # set running trials; You can trial run_trial = 5 to debug\n",
    "    run_trial = 10\n",
    "\n",
    "    # run experiment for the Tile-based method\n",
    "    results_tile = []\n",
    "    for _ in range(run_trial):        \n",
    "        # create the environment\n",
    "        my_env = FourRooms()\n",
    "\n",
    "        # run semi-gradient SARSA with Tile-based method with tile size n = 2\n",
    "        my_agent = TileAgent(my_env, params)\n",
    "        res = my_agent.run()\n",
    "\n",
    "        # save result for each running trial\n",
    "        results_tile.append(np.array(res))\n",
    "        \n",
    "    # run experiment for the Room-based method\n",
    "    results_room = []\n",
    "    for _ in range(run_trial):        \n",
    "        # create the environment\n",
    "        my_env = FourRooms()\n",
    "\n",
    "        # run semi-gradient SARSA with Room-based method\n",
    "        my_agent = RoomAgent(my_env, params)\n",
    "        res = my_agent.run()\n",
    "\n",
    "        # save result for each running trial\n",
    "        results_room.append(np.array(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205517c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plot_curves([np.array(results), np.array(results_tile), np.array(results_room)],\n",
    "            [\"State-aggregation: identical\", \"State-aggregation: Tile = 2x2\", \"State aggregation: Room-based\"],\n",
    "            [\"b\", \"r\", \"g\"],\n",
    "            \"Averaged discounted return\", \"Q3 - (b): Comparison between three state aggregation strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8161caab",
   "metadata": {},
   "source": [
    "# Q3 - (d):  Adapt your implementation of semi-gradient one-step SARSA for linear function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eefc427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implement your code here.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475f601f",
   "metadata": {},
   "source": [
    "# Q3 - (e) [5180]: Implement the following two features, and plot the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4008d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implement your code here.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2f778085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"The distance matrix is here\"\"\"\n",
    "\n",
    "distance_matrix = np.array([[14, 13, 12, 11, 10, -1,  4,  3,  2,  1,  0],\n",
    "                            [13, 12, 11, 10,  9, -1,  5,  4,  3,  2,  1],\n",
    "                            [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],\n",
    "                            [13, 12, 11, 10,  9, -1,  7,  6,  5,  4,  3],\n",
    "                            [14, 13, 12, 11, 10, -1,  8,  7,  6,  5,  4],\n",
    "                            [-1, 14, -1, -1, -1, -1,  9,  8,  7,  6,  5],\n",
    "                            [16, 15, 16, 17, 18, -1, -1, -1,  8, -1, -1],\n",
    "                            [17, 16, 17, 18, 17, -1, 11, 10,  9, 10, 11],\n",
    "                            [18, 17, 18, 17, 16, -1, 12, 11, 10, 11, 12],\n",
    "                            [19, 18, 17, 16, 15, 14, 13, 12, 11, 12, 13],\n",
    "                            [20, 19, 18, 17, 16, -1, 14, 13, 12, 13, 14]])\n",
    "plt.imshow(distance_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339ab5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
